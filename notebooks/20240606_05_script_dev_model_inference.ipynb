{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Pipeline Development\n",
    "- Author: Marcellinus Aditya Witarsah\n",
    "- Date: 06 June 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from scipy import stats\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "from src.utils.common import logger\n",
    "from src.utils.common import read_yaml, create_directories\n",
    "from src.constants import CONFIG_FILE_PATH, SCHEMA_FILE_PATH, PARAMS_FILE_PATH\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from optbinning import Scorecard\n",
    "# from optbinning import BinningProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find way to deploy model from dagshub: using docker and fetch API request.\n",
    "2. Using request to get model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up environment variables:\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"../artifacts/data_preprocessing/test.csv\")\n",
    "X_test, y_test = test.drop(columns=[\"loan_status\"]),test[\"loan_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 9/9 [00:00<00:00, 16.47it/s] \n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "# get last experiment id\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/marcellinus-witarsah/credit-scorecard-modelling.mlflow\")\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.sklearn.load_model(\"models:/WeightOfEvidence+LogisticRegression/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([572.08448915])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.score(X_test.iloc[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/75534090/mlflow-model-serve-cant-find-pyenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person_age': 22,\n",
       " 'person_income': 50000,\n",
       " 'person_home_ownership': 'RENT',\n",
       " 'person_emp_length': 6.0,\n",
       " 'loan_intent': 'PERSONAL',\n",
       " 'loan_grade': 'B',\n",
       " 'loan_amnt': 6000,\n",
       " 'loan_int_rate': 11.89,\n",
       " 'loan_percent_income': 0.12,\n",
       " 'cb_person_default_on_file': 'N',\n",
       " 'cb_person_cred_hist_length': 2,\n",
       " 'loan_status': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"../artifacts/data_preprocessing/test.csv\")\n",
    "test.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "ENDPOINT=\"http://127.0.0.1:5001/invocations\"\n",
    "headers={\"Content-Type\": \"application/json\"}\n",
    "prediction=requests.post(\n",
    "    url=ENDPOINT,\n",
    "    json={\n",
    "        'inputs': {\n",
    "            'person_age': 22,\n",
    "            'person_income': 50000,\n",
    "            'person_home_ownership': 'RENT',\n",
    "            'person_emp_length': 6.0,\n",
    "            'loan_intent': 'PERSONAL',\n",
    "            'loan_grade': 'B',\n",
    "            'loan_amnt': 6000,\n",
    "            'loan_int_rate': 11.89,\n",
    "            'loan_percent_income': 0.12,\n",
    "            'cb_person_default_on_file': 'N',\n",
    "            'cb_person_cred_hist_length': 2,\n",
    "        }\n",
    "    },\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "prediction = json.loads(prediction.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# src/entities/config_entity.py\n",
    "@dataclass(frozen=True)\n",
    "class ModelInferenceConfig:\n",
    "    \"\"\"\n",
    "    Dataclass for storing model inference configuration.\n",
    "\n",
    "    This class provides a type-safe way to store configuration parameters for model inference, \n",
    "    ensuring that the specified attributes are immutable once set.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): The root directory for model inference artifacts.\n",
    "        model_path (Path): The path to the model file.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    model_path: Path\n",
    "\n",
    "\n",
    "# src/config/configuration_manager.py\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Class to manage and prepare configuration settings for the pipeline.\n",
    "\n",
    "    This class is responsible for reading configuration files and preparing\n",
    "    configuration settings for the pipeline.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Parsed configuration file content.\n",
    "        params (dict): Parsed parameters file content.\n",
    "        schema (dict): Parsed schema file content.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: str = CONFIG_FILE_PATH, \n",
    "        params_filepath: str = PARAMS_FILE_PATH, \n",
    "        schema_filepath: str = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ConfigurationManager with file paths.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): File path to the configuration YAML file.\n",
    "            params_filepath (str): File path to the parameters YAML file.\n",
    "            schema_filepath (str): File path to the schema YAML file.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(Path(config_filepath))\n",
    "        self.params = read_yaml(Path(params_filepath))\n",
    "        self.schema = read_yaml(Path(schema_filepath))\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_inference_config(self):\n",
    "        \"\"\"\n",
    "        Get the configuration for model inference.\n",
    "\n",
    "        This method reads the model inference configuration from the config \n",
    "        file and prepares the directories required for model inference.\n",
    "\n",
    "        Returns:\n",
    "            ModelInferenceConfig: An instance of ModelInferenceConfig containing \n",
    "            the root directory and model path for model inference.\n",
    "        \"\"\"\n",
    "        config = self.config.model_inference\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        model_inference_config = ModelInferenceConfig(\n",
    "            root_dir = Path(config.root_dir),\n",
    "            model_path = Path(config.model_path),\n",
    "        )\n",
    "        \n",
    "        return model_inference_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - Created directory at: models/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelInferenceConfig(root_dir=WindowsPath('models'), model_path=WindowsPath('models/model.joblib'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration_manager = ConfigurationManager()\n",
    "configuration_manager.get_model_inference_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lates model\n",
    "print_models_info(client.get_latest_versions(name, stages=[\"None\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "class ModelInference:\n",
    "    \"\"\"\n",
    "    A class used to perform model inference using a pre-trained model.\n",
    "\n",
    "    This class is responsible for loading a model from a specified path and\n",
    "    providing methods to make predictions on input data.\n",
    "\n",
    "    Attributes:\n",
    "        config (ModelInferenceConfig): Configuration for model inference.\n",
    "        model: The loaded machine learning model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ModelInferenceConfig):\n",
    "        \"\"\"\n",
    "        Initialize the ModelInference with a configuration.\n",
    "\n",
    "        Args:\n",
    "            config (ModelInferenceConfig): The configuration containing paths for model inference.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.model = self.get_model()\n",
    "        \n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Load the model from the file specified in the configuration.\n",
    "\n",
    "        This method reads the model file from the path specified in the config\n",
    "        and loads it into memory.\n",
    "\n",
    "        Returns:\n",
    "            model: The loaded machine learning model.\n",
    "        \"\"\"\n",
    "        logger.info(\"Load model\")\n",
    "        model = None        \n",
    "        with open(self.config.model_path, 'rb') as f:\n",
    "            model = joblib.load(f)\n",
    "        return model\n",
    "    \n",
    "    def predict(self, data: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make predictions on input data.\n",
    "\n",
    "        Args:\n",
    "            data (np.array): Preprocessed input data for which predictions are to be made.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The predicted values.\n",
    "        \"\"\"\n",
    "        logger.info(\"Predict\")\n",
    "        prediction = self.model.predict(data)\n",
    "        return prediction\n",
    "    \n",
    "    def predict_proba(self, data: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make probability predictions on input data.\n",
    "\n",
    "        Args:\n",
    "            data (np.array): Preprocessed input data for which probability predictions are to be made.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The predicted probabilities.\n",
    "        \"\"\"\n",
    "        logger.info(\"Predict probabilities\")\n",
    "        prediction = self.model.predict_proba(data)\n",
    "        return prediction[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - Created directory at: models/\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - Load model\n",
      "2024-06-06 09:46:53,779 - credit-scorecard-logger - INFO - Predict probabilities\n",
      "[0.07598039 0.05261978 0.01292163 ... 0.06346361 0.12475196 0.02963603]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    model_inference = ModelInference(configuration_manager.get_model_inference_config())\n",
    "    data = pd.read_csv(\"artifacts/data_preprocessing/test.csv\")\n",
    "    X = data.drop(columns=['loan_status'])\n",
    "    prediction = model_inference.predict_proba(X)\n",
    "    print(prediction)\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Restart and run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.utils.common import logger\n",
    "from src.config.configuration_manager import ConfigurationManager\n",
    "from src.models.model_inference import ModelInference\n",
    "from typing import Union\n",
    "\n",
    "class ModelInferencePipeline:\n",
    "    \"\"\"\n",
    "    A pipeline class for running model inference.\n",
    "\n",
    "    This class is responsible for setting up the configuration and \n",
    "    model inference components and running predictions on input data.\n",
    "\n",
    "    Attributes:\n",
    "        configuration_manager (ConfigurationManager): Manages the configuration settings.\n",
    "        model_inference_config (ModelInferenceConfig): Configuration for model inference.\n",
    "        model_inference (ModelInference): Instance of ModelInference to make predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Instantiate the ModelInferencePipeline class.\n",
    "        \"\"\"\n",
    "        self.configuration_manager = ConfigurationManager()\n",
    "        self.model_inference_config = (\n",
    "            self.configuration_manager.get_model_inference_config()\n",
    "        )\n",
    "        self.model_inference = ModelInference(self.model_inference_config)\n",
    "\n",
    "    def run(self, data: Union[pd.DataFrame, np.array]) -> np.array:\n",
    "        \"\"\"\n",
    "        Run the model inference pipeline on input data.\n",
    "\n",
    "        Args:\n",
    "            data (Union[pd.DataFrame, np.array]): The input data for prediction.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The predicted probabilities.\n",
    "        \"\"\"\n",
    "        prediction = self.model_inference.predict_proba(data)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:57:21,813 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-06 09:57:21,862 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-06 09:57:21,866 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-06 09:57:21,867 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-06 09:57:21,869 - credit-scorecard-logger - INFO - Created directory at: models/\n",
      "2024-06-06 09:57:21,870 - credit-scorecard-logger - INFO - Load model\n",
      "2024-06-06 09:57:24,069 - credit-scorecard-logger - INFO - Predict probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.07598039, 0.05261978, 0.01292163, ..., 0.06346361, 0.12475196,\n",
       "       0.02963603])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"artifacts/data_preprocessing/test.csv\")\n",
    "model_inference_pipeline = ModelInferencePipeline()\n",
    "model_inference_pipeline.run(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
