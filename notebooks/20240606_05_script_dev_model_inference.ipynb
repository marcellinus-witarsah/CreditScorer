{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Pipeline Development\n",
    "- Author: Marcellinus Aditya Witarsah\n",
    "- Date: 06 June 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from scipy import stats\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "from src.utils.common import logger\n",
    "from src.utils.common import read_yaml, create_directories\n",
    "from src.constants import CONFIG_FILE_PATH, SCHEMA_FILE_PATH, PARAMS_FILE_PATH\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "# from optbinning import Scorecard\n",
    "# from optbinning import BinningProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run once only\n",
    "# os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelInferenceConfig:\n",
    "    \"\"\"\n",
    "    Data class for storing model inference configuration.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (str): Root directory for model inference.\n",
    "        model_path (str): Path to save the trained model.\n",
    "        train_data_path (str): Path to the inference data.\n",
    "        test_data_path (str): Path to the test data.\n",
    "        experiment_name (str): Name of the experiment.\n",
    "        registered_model_name (str): Model name.\n",
    "        target_column (str): The name of the target column.\n",
    "        binning_process (dict): Configuration for the binning process.\n",
    "        logistic_regression (dict): Configuration for logistic regression.\n",
    "        scorecard (dict): Configuration for the scorecard.\n",
    "    \"\"\"\n",
    "    root_dir: str\n",
    "    model_path: str\n",
    "    train_data_path: str\n",
    "    test_data_path: str\n",
    "    experiment_name: str\n",
    "    registered_model_name: str\n",
    "    target_column: str\n",
    "    binning_process: dict\n",
    "    logistic_regression: dict\n",
    "    scorecard: dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find way to deploy model from dagshub: using docker and fetch API request.\n",
    "2. Using request to get model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up environment variables:\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# get last experiment id\n",
    "mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=credit-score-model; run_id=5c0e9a3c75e647439c3fd12b407d4c23; version=1, stage=None\n",
      "name=WeightOfEvidence+LogisticRegression; run_id=e6157b63b09b446399ca05ea50f9c438; version=3, stage=None\n",
      "name=woe-lr; run_id=d79bc9c897f042b1b81dee446f38fbd2; version=1, stage=None\n"
     ]
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "for model in client.search_registered_models(filter_string=\"name LIKE '%'\"):\n",
    "    for model_version in model.latest_versions:\n",
    "        print(f\"name={model_version.name}; run_id={model_version.run_id}; version={model_version.version}, stage={model_version.current_stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str, version: int) -> BaseEstimator:\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI'))\n",
    "        model = mlflow.sklearn.load_model(f\"models:/{model_name}/{version}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\923006079\\AppData\\Local\\anaconda3\\envs\\credit-risk-modelling\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 9/9 [00:00<00:00, 13.88it/s]\n",
      "c:\\Users\\923006079\\AppData\\Local\\anaconda3\\envs\\credit-risk-modelling\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([572.08448915, 583.40533029, 625.106093  , ..., 577.6666715 ,\n",
       "       556.21258813, 600.6620341 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model:\n",
    "model = load_model(\"credit-score-model\", 1)\n",
    "\n",
    "# Test model:\n",
    "test = pd.read_csv(\"../artifacts/data_preprocessing/test.csv\")\n",
    "X_test, y_test = test.drop(columns=['loan_status']), test['loan_status']\n",
    "model.score(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person_age': 22,\n",
       " 'person_income': 50000,\n",
       " 'person_home_ownership': 'RENT',\n",
       " 'person_emp_length': 6.0,\n",
       " 'loan_intent': 'PERSONAL',\n",
       " 'loan_grade': 'B',\n",
       " 'loan_amnt': 6000,\n",
       " 'loan_int_rate': 11.89,\n",
       " 'loan_percent_income': 0.12,\n",
       " 'cb_person_default_on_file': 'N',\n",
       " 'cb_person_cred_hist_length': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [500]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "ENDPOINT=\"http://127.0.0.1:8000/credit-score\"\n",
    "input = {\n",
    "    'person_age': 22,\n",
    "    'person_income': 50000,\n",
    "    'person_home_ownership': 'RENT',\n",
    "    'person_emp_length': 6.0,\n",
    "    'loan_intent': 'PERSONAL',\n",
    "    'loan_grade': 'B',\n",
    "    'loan_amnt': 6000,\n",
    "    'loan_int_rate': 11.89,\n",
    "    'loan_percent_income': 0.12,\n",
    "    'cb_person_default_on_file': 'N',\n",
    "    'cb_person_cred_hist_length': 2\n",
    "}\n",
    "prediction = requests.post(\n",
    "    url=ENDPOINT,\n",
    "    json=input,\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "prediction #.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# src/entities/config_entity.py\n",
    "@dataclass(frozen=True)\n",
    "class ModelInferenceConfig:\n",
    "    \"\"\"\n",
    "    Dataclass for storing model inference configuration.\n",
    "\n",
    "    This class provides a type-safe way to store configuration parameters for model inference, \n",
    "    ensuring that the specified attributes are immutable once set.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): The root directory for model inference artifacts.\n",
    "        model_path (Path): The path to the model file.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    model_path: Path\n",
    "\n",
    "\n",
    "# src/config/configuration_manager.py\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Class to manage and prepare configuration settings for the pipeline.\n",
    "\n",
    "    This class is responsible for reading configuration files and preparing\n",
    "    configuration settings for the pipeline.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Parsed configuration file content.\n",
    "        params (dict): Parsed parameters file content.\n",
    "        schema (dict): Parsed schema file content.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: str = CONFIG_FILE_PATH, \n",
    "        params_filepath: str = PARAMS_FILE_PATH, \n",
    "        schema_filepath: str = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ConfigurationManager with file paths.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): File path to the configuration YAML file.\n",
    "            params_filepath (str): File path to the parameters YAML file.\n",
    "            schema_filepath (str): File path to the schema YAML file.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(Path(config_filepath))\n",
    "        self.params = read_yaml(Path(params_filepath))\n",
    "        self.schema = read_yaml(Path(schema_filepath))\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_inference_config(self):\n",
    "        \"\"\"\n",
    "        Get the configuration for model inference.\n",
    "\n",
    "        This method reads the model inference configuration from the config \n",
    "        file and prepares the directories required for model inference.\n",
    "\n",
    "        Returns:\n",
    "            ModelInferenceConfig: An instance of ModelInferenceConfig containing \n",
    "            the root directory and model path for model inference.\n",
    "        \"\"\"\n",
    "        config = self.config.model_inference\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        model_inference_config = ModelInferenceConfig(\n",
    "            root_dir = Path(config.root_dir),\n",
    "            model_path = Path(config.model_path),\n",
    "        )\n",
    "        \n",
    "        return model_inference_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-06 09:42:17,862 - credit-scorecard-logger - INFO - Created directory at: models/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelInferenceConfig(root_dir=WindowsPath('models'), model_path=WindowsPath('models/model.joblib'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration_manager = ConfigurationManager()\n",
    "configuration_manager.get_model_inference_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "class ModelInference:\n",
    "    \"\"\"\n",
    "    A class used to perform model inference using a pre-trained model.\n",
    "\n",
    "    This class is responsible for loading a model from a specified path and\n",
    "    providing methods to make predictions on input data.\n",
    "\n",
    "    Attributes:\n",
    "        config (ModelInferenceConfig): Configuration for model inference.\n",
    "        model: The loaded machine learning model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ModelInferenceConfig):\n",
    "        \"\"\"\n",
    "        Initialize the ModelInference with a configuration.\n",
    "\n",
    "        Args:\n",
    "            config (ModelInferenceConfig): The configuration containing paths for model inference.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.model = self.get_model()\n",
    "        \n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Load the model from the file specified in the configuration.\n",
    "\n",
    "        This method reads the model file from the path specified in the config\n",
    "        and loads it into memory.\n",
    "\n",
    "        Returns:\n",
    "            model: The loaded machine learning model.\n",
    "        \"\"\"\n",
    "        logger.info(\"Load model\")\n",
    "        model = None        \n",
    "        with open(self.config.model_path, 'rb') as f:\n",
    "            model = joblib.load(f)\n",
    "        return model\n",
    "    \n",
    "    def predict(self, data: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make predictions on input data.\n",
    "\n",
    "        Args:\n",
    "            data (np.array): Preprocessed input data for which predictions are to be made.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The predicted values.\n",
    "        \"\"\"\n",
    "        logger.info(\"Predict\")\n",
    "        prediction = self.model.predict(data)\n",
    "        return prediction\n",
    "    \n",
    "    def predict_proba(self, data: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Make probability predictions on input data.\n",
    "\n",
    "        Args:\n",
    "            data (np.array): Preprocessed input data for which probability predictions are to be made.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The predicted probabilities.\n",
    "        \"\"\"\n",
    "        logger.info(\"Predict probabilities\")\n",
    "        prediction = self.model.predict_proba(data)\n",
    "        return prediction[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - Created directory at: models/\n",
      "2024-06-06 09:46:53,736 - credit-scorecard-logger - INFO - Load model\n",
      "2024-06-06 09:46:53,779 - credit-scorecard-logger - INFO - Predict probabilities\n",
      "[0.07598039 0.05261978 0.01292163 ... 0.06346361 0.12475196 0.02963603]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    model_inference = ModelInference(configuration_manager.get_model_inference_config())\n",
    "    data = pd.read_csv(\"artifacts/data_preprocessing/test.csv\")\n",
    "    X = data.drop(columns=['loan_status'])\n",
    "    prediction = model_inference.predict_proba(X)\n",
    "    print(prediction)\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Restart and run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.utils.common import logger\n",
    "from src.config.configuration_manager import ConfigurationManager\n",
    "from src.models.model_inference import ModelInference\n",
    "from typing import Union\n",
    "\n",
    "class ModelInferencePipeline:\n",
    "    \"\"\"\n",
    "    A pipeline class for running model inference.\n",
    "\n",
    "    This class is responsible for setting up the configuration and \n",
    "    model inference components and running predictions on input data.\n",
    "\n",
    "    Attributes:\n",
    "        configuration_manager (ConfigurationManager): Manages the configuration settings.\n",
    "        model_inference_config (ModelInferenceConfig): Configuration for model inference.\n",
    "        model_inference (ModelInference): Instance of ModelInference to make predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Instantiate the ModelInferencePipeline class.\n",
    "        \"\"\"\n",
    "        self.configuration_manager = ConfigurationManager()\n",
    "        self.model_inference_config = (\n",
    "            self.configuration_manager.get_model_inference_config()\n",
    "        )\n",
    "        self.model_inference = ModelInference(self.model_inference_config)\n",
    "\n",
    "    def run(self, data: Union[pd.DataFrame, np.array]) -> np.array:\n",
    "        \"\"\"\n",
    "        Run the model inference pipeline on input data.\n",
    "\n",
    "        Args:\n",
    "            data (Union[pd.DataFrame, np.array]): The input data for prediction.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The predicted probabilities.\n",
    "        \"\"\"\n",
    "        prediction = self.model_inference.predict_proba(data)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:57:21,813 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-06 09:57:21,862 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-06 09:57:21,866 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-06 09:57:21,867 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-06 09:57:21,869 - credit-scorecard-logger - INFO - Created directory at: models/\n",
      "2024-06-06 09:57:21,870 - credit-scorecard-logger - INFO - Load model\n",
      "2024-06-06 09:57:24,069 - credit-scorecard-logger - INFO - Predict probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.07598039, 0.05261978, 0.01292163, ..., 0.06346361, 0.12475196,\n",
       "       0.02963603])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"artifacts/data_preprocessing/test.csv\")\n",
    "model_inference_pipeline = ModelInferencePipeline()\n",
    "model_inference_pipeline.run(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
