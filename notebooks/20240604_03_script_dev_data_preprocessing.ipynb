{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline Development\n",
    "- Author: Marcellinus Aditya Witarsah\n",
    "- Date: 05 June 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from scipy import stats\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "from src.utils.common import logger\n",
    "from src.utils.common import read_yaml, create_directories\n",
    "from src.constants import CONFIG_FILE_PATH, SCHEMA_FILE_PATH, PARAMS_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/entities/config_entity.py\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    \"\"\"\n",
    "    Data class for storing data preprocessing configuration.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): Root directory for data preprocessing.\n",
    "        source_path (Path): Source path of the data to be processed.\n",
    "        train_data_path (Path): Path to save the training data.\n",
    "        test_data_path (Path): Path to save the testing data.\n",
    "        target_column (str): The name of the target column.\n",
    "        test_size (float): The proportion of the dataset to include in the test split.\n",
    "        shuffle (bool): Whether or not to shuffle the data before splitting.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    source_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    target_column: str\n",
    "    test_size: float\n",
    "    shuffle: bool\n",
    "    random_state: int\n",
    "\n",
    "# src/config/configuration_manager.py\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Prepare ConfigurationManager class.\n",
    "    \n",
    "    This class is responsible for reading configuration files and preparing\n",
    "    configuration settings for the pipeline.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Parsed configuration file content.\n",
    "        params (dict): Parsed parameters file content.\n",
    "        schema (dict): Parsed schema file content.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: str = CONFIG_FILE_PATH, \n",
    "        params_filepath: str = PARAMS_FILE_PATH, \n",
    "        schema_filepath: str = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ConfigurationManager with file paths.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): File path to the configuration YAML file.\n",
    "            params_filepath (str): File path to the parameters YAML file.\n",
    "            schema_filepath (str): File path to the schema YAML file.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(Path(config_filepath))\n",
    "        self.params = read_yaml(Path(params_filepath))\n",
    "        self.schema = read_yaml(Path(schema_filepath))\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:\n",
    "        \"\"\"\n",
    "        Get configuration for data preprocessing.\n",
    "        \n",
    "        Returns:\n",
    "            DataPreprocessingConfig: Configuration for data preprocessing.\n",
    "        \"\"\"\n",
    "        config = self.config.data_preprocessing\n",
    "        params = self.params.data_preprocessing\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocessing_config = DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_path=config.source_path,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            target_column=params.split_data.target_column,\n",
    "            test_size=params.split_data.test_size,\n",
    "            shuffle=params.split_data.shuffle,\n",
    "            random_state=params.split_data.random_state\n",
    "        )\n",
    "        return data_preprocessing_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "# src/data/data_preprocessing.py\n",
    "class DataPreprocessing:\n",
    "    \"\"\"\n",
    "    Class to handle the data preprocessing process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        \"\"\"\n",
    "        Instantiate `DataPreprocessing` class.\n",
    "\n",
    "        Args:\n",
    "            config (DataPreprocessingConfig): Configuration for data preprocessing.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def split_data(self) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Split data into train and test data evenly based on their target values.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]: Train and test set.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Split data\")\n",
    "            df = pd.read_csv(self.config.source_path)\n",
    "            X, y = df.drop(columns=[self.config.target_column]), df[self.config.target_column]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, \n",
    "                stratify=y, \n",
    "                test_size=self.config.test_size, \n",
    "                shuffle=self.config.shuffle, \n",
    "                random_state=self.config.random_state\n",
    "            )\n",
    "            train = pd.concat([X_train, y_train], axis=1)\n",
    "            test = pd.concat([X_test, y_test], axis=1)\n",
    "            train.to_csv(self.config.train_data_path, index=False)\n",
    "            test.to_csv(self.config.test_data_path, index=False)\n",
    "        except Exception as e:\n",
    "            logger.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-05 15:25:47,159 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-05 15:25:47,164 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-05 15:25:47,167 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-05 15:25:47,168 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-05 15:25:47,170 - credit-scorecard-logger - INFO - Created directory at: artifacts/data_preprocessing\n",
      "2024-06-05 15:25:47,171 - credit-scorecard-logger - INFO - Split data\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    data_preprocessing = DataPreprocessing( \n",
    "        config=configuration_manager.get_data_preprocessing_config()\n",
    "    )\n",
    "    data_preprocessing.split_data()\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Restart and run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-05 15:28:50,967 - credit-scorecard-logger - INFO - >>>>>> Data preprocessing Stage Started <<<<<<\n",
      "2024-06-05 15:28:50,973 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-05 15:28:50,978 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-05 15:28:50,981 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-05 15:28:50,985 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-05 15:28:50,986 - credit-scorecard-logger - INFO - Created directory at: artifacts/data_preprocessing\n",
      "2024-06-05 15:28:50,987 - credit-scorecard-logger - INFO - Split data\n",
      "2024-06-05 15:28:51,290 - credit-scorecard-logger - INFO - >>>>>> Data preprocessing Stage Completed <<<<<<\n"
     ]
    }
   ],
   "source": [
    "from src.utils.common import logger\n",
    "from src.config.configuration_manager import ConfigurationManager\n",
    "from src.data.data_preprocessing import DataPreprocessing\n",
    "\n",
    "\n",
    "class DataPreprocessingPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Instantiate `DataPreprocessingPipeline` class\n",
    "        \"\"\"\n",
    "        self.configuration_manager = ConfigurationManager()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Preprocess data\n",
    "        \"\"\"\n",
    "        data_preprocessing = DataPreprocessing(\n",
    "            config=self.configuration_manager.get_data_preprocessing_config()\n",
    "        )\n",
    "        data_preprocessing.split_data()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    STAGE_NAME = \"Data Preprocessing Stage\"\n",
    "    try:\n",
    "        logger.info(f\">>>>>> {STAGE_NAME} Started <<<<<<\")\n",
    "        data_preprocessing_pipeline = DataPreprocessingPipeline()\n",
    "        data_preprocessing_pipeline.run()\n",
    "        logger.info(f\">>>>>> {STAGE_NAME} Completed <<<<<<\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
