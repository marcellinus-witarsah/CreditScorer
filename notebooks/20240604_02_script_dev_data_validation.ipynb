{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation Pipeline Development\n",
    "- Author: Marcellinus Aditya Witarsah\n",
    "- Date: 05 June 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from scipy import stats\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "from src.utils.common import logger\n",
    "from src.utils.common import read_yaml, create_directories\n",
    "from src.constants import CONFIG_FILE_PATH, SCHEMA_FILE_PATH, PARAMS_FILE_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/entities/config_entity.py\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    \"\"\"\n",
    "    Data class for storing data validation configuration.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): Root directory for data validation.\n",
    "        source_path (Path): Source path of the data to be validated.\n",
    "        STATUS_FILE (Path): Path to the status file.\n",
    "        schema (list): List defining the schema for validation.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    source_path: Path\n",
    "    STATUS_FILE: Path\n",
    "    schema: list \n",
    "\n",
    "# src/config/configuration_manager.py\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Prepare ConfigurationManager class.\n",
    "    \n",
    "    This class is responsible for reading configuration files and preparing\n",
    "    configuration settings for the pipeline.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Parsed configuration file content.\n",
    "        params (dict): Parsed parameters file content.\n",
    "        schema (dict): Parsed schema file content.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: str = CONFIG_FILE_PATH, \n",
    "        params_filepath: str = PARAMS_FILE_PATH, \n",
    "        schema_filepath: str = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ConfigurationManager with file paths.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): File path to the configuration YAML file.\n",
    "            params_filepath (str): File path to the parameters YAML file.\n",
    "            schema_filepath (str): File path to the schema YAML file.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(Path(config_filepath))\n",
    "        self.params = read_yaml(Path(params_filepath))\n",
    "        self.schema = read_yaml(Path(schema_filepath))\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "        Get configuration for data validation.\n",
    "        \n",
    "        Returns:\n",
    "            DataValidationConfig: Configuration for data validation.\n",
    "        \"\"\"\n",
    "        config = self.config.data_validation\n",
    "        schema = self.schema.COLUMNS\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_path=config.source_path,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            schema=schema\n",
    "        )\n",
    "        return data_validation_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data/data_validation.py\n",
    "class DataValidation:\n",
    "    \"\"\"\n",
    "    Class to handle the data validation process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        \"\"\"\n",
    "        Instantiate `DataValidation` class.\n",
    "\n",
    "        Args:\n",
    "            config (DataValidationConfig): Configuration for data validation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def validate_data(self):\n",
    "        \"\"\"\n",
    "        Validate the data based on the provided schema.\n",
    "        \n",
    "        This method reads a CSV file from the source path and checks if all columns match the schema.\n",
    "        \n",
    "        Logs messages indicating whether data types match or not.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Validate data\")\n",
    "            validation_status = None\n",
    "            \n",
    "            df = pd.read_csv(self.config.source_path)\n",
    "            all_cols = df.columns\n",
    "            all_schema = self.config.schema\n",
    "            \n",
    "            for col in all_cols:\n",
    "                if col not in all_schema.keys():\n",
    "                    validation_status = False\n",
    "                else:\n",
    "                    if df[col].dtype == all_schema[col]:\n",
    "                        validation_status = True\n",
    "                    else:\n",
    "                        validation_status = False\n",
    "                        \n",
    "            if validation_status:\n",
    "                logger.info(\"All data types match\")\n",
    "            else:\n",
    "                logger.info(\"There's a data types mismatch\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-05 14:34:05,724 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-05 14:34:05,727 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-05 14:34:05,731 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-05 14:34:05,733 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-05 14:34:05,734 - credit-scorecard-logger - INFO - Created directory at: artifacts/data_validation\n",
      "2024-06-05 14:34:05,735 - credit-scorecard-logger - INFO - Validate data\n",
      "2024-06-05 14:34:05,779 - credit-scorecard-logger - INFO - All data types match\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    data_ingestion = DataValidation( \n",
    "        config=configuration_manager.get_data_validation_config()\n",
    "    )\n",
    "    data_ingestion.validate_data()\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Restart and run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-05 14:31:03,263 - credit-scorecard-logger - INFO - >>>>>> Data Validation Stage Started <<<<<<\n",
      "2024-06-05 14:31:03,268 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-05 14:31:03,272 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-05 14:31:03,276 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-05 14:31:03,278 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-05 14:31:03,280 - credit-scorecard-logger - INFO - Created directory at: artifacts/data_validation\n",
      "2024-06-05 14:31:03,281 - credit-scorecard-logger - INFO - Validate data\n",
      "2024-06-05 14:31:03,329 - credit-scorecard-logger - INFO - All data types are match\n",
      "2024-06-05 14:31:03,330 - credit-scorecard-logger - INFO - >>>>>> Data Validation Stage Completed <<<<<<\n"
     ]
    }
   ],
   "source": [
    "from src.utils.common import logger\n",
    "from src.config.configuration import ConfigurationManager\n",
    "from src.data.data_validation import DataValidation\n",
    "\n",
    "\n",
    "class DataValidationPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Instantiate `DataValidationPipeline` class\n",
    "        \"\"\"\n",
    "        self.configuration_manager = ConfigurationManager()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Validate data\n",
    "        \"\"\"\n",
    "        data_validation = DataValidation(\n",
    "            config=self.configuration_manager.get_data_validation_config()\n",
    "        )\n",
    "        data_validation.validate_data()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    STAGE_NAME = \"Data Validation Stage\"\n",
    "    try:\n",
    "        logger.info(f\">>>>>> {STAGE_NAME} Started <<<<<<\")\n",
    "        data_validation_training_pipeline = DataValidationPipeline()\n",
    "        data_validation_training_pipeline.run()\n",
    "        logger.info(f\">>>>>> {STAGE_NAME} Completed <<<<<<\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
