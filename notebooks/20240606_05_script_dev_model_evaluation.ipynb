{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Pipeline Development\n",
    "- Author: Marcellinus Aditya Witarsah\n",
    "- Date: 05 June 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from scipy import stats\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "from src.utils.common import logger\n",
    "from src.utils.common import read_yaml, create_directories\n",
    "from src.constants import CONFIG_FILE_PATH, SCHEMA_FILE_PATH, PARAMS_FILE_PATH\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from optbinning import Scorecard\n",
    "from optbinning import BinningProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once only\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# src/entities/config_entity.py\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    \"\"\"\n",
    "    Data class for storing model training configuration.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): Root directory for model training.\n",
    "        train_data_path (Path): Path to the training data.\n",
    "        model_path (Path): Path to save the trained model.\n",
    "        target_column (str): The name of the target column.\n",
    "        BinningProcess (dict): Configuration for the binning process.\n",
    "        LogisticRegression (dict): Configuration for logistic regression.\n",
    "        Scorecard (dict): Configuration for the scorecard.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    mlflow_uri: str\n",
    "    mlflow_username: str\n",
    "    mlflow_password: str\n",
    "\n",
    "# src/config/configuration_manager.py\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Prepare ConfigurationManager class.\n",
    "    \n",
    "    This class is responsible for reading configuration files and preparing\n",
    "    configuration settings for the pipeline.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Parsed configuration file content.\n",
    "        params (dict): Parsed parameters file content.\n",
    "        schema (dict): Parsed schema file content.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: str = CONFIG_FILE_PATH, \n",
    "        params_filepath: str = PARAMS_FILE_PATH, \n",
    "        schema_filepath: str = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ConfigurationManager with file paths.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): File path to the configuration YAML file.\n",
    "            params_filepath (str): File path to the parameters YAML file.\n",
    "            schema_filepath (str): File path to the schema YAML file.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(Path(config_filepath))\n",
    "        self.params = read_yaml(Path(params_filepath))\n",
    "        self.schema = read_yaml(Path(schema_filepath))\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \"\"\"\n",
    "        Get configuration for model training.\n",
    "        \n",
    "        Returns:\n",
    "            ModelEvaluationConfig: Configuration for model training.\n",
    "        \"\"\"\n",
    "        config = self.config.model_evaluation\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_path=config.model_path,\n",
    "            metric_file_name=config.metric_file_name,\n",
    "            target_column=schema.name,\n",
    "            mlflow_uri=os.getenv(\"MLFLOW_TRAKING_URI\"),\n",
    "            mlflow_username=os.getenv(\"MLFLOW_TRACKING_USERNAME\"),\n",
    "            mlflow_password=os.getenv(\"MLFLOW_TRACKING_PASSWORD\")\n",
    "        )\n",
    "        return model_evaluation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:21:53,421 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-06 09:21:53,426 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-06 09:21:53,428 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-06 09:21:53,428 - credit-scorecard-logger - INFO - Created directory at: artifacts\n"
     ]
    }
   ],
   "source": [
    "configuration_manager = ConfigurationManager()\n",
    "# configuration_manager.get_model_evaluation_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Tuple\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import mlflow\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "\n",
    "# src/data/model_training.py\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def roc_auc(self, y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate ROC AUC (Area Under the Receiver Operating Characteristic Curve).\n",
    "        \n",
    "        Args:\n",
    "            y_true (Union[list, np.array]): True labels.\n",
    "            y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "        Returns:\n",
    "            float: ROC AUC score.\n",
    "        \"\"\"\n",
    "        return roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    def pr_auc(self, y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate PR AUC (Area Under the Precision Recall Curve).\n",
    "        \n",
    "        Args:\n",
    "            y_true (Union[list, np.array]): True labels.\n",
    "            y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "        Returns:\n",
    "            float: PR AUC score.\n",
    "        \"\"\"\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "    def gini(self, y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Gini coefficient.\n",
    "\n",
    "        Args:\n",
    "            y_true (Union[list, np.array]): True labels.\n",
    "            y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "        Returns:\n",
    "            float: Gini coefficient.\n",
    "        \"\"\"\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        return 2 * roc_auc - 1\n",
    "\n",
    "    def ks(self, y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Kolmogorov-Smirnov (KS) statistic.\n",
    "\n",
    "        Args:\n",
    "            y_true (Union[list, np.array]): True labels.\n",
    "            y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "        Returns:\n",
    "            float: KS statistic.\n",
    "        \"\"\"\n",
    "        y_pred_proba_not_default = y_pred_proba[y_true == 0]\n",
    "        y_pred_proba_default = y_pred_proba[y_true == 1]\n",
    "        ks_stat, p_value = stats.ks_2samp(y_pred_proba_not_default, y_pred_proba_default)\n",
    "        return ks_stat\n",
    "\n",
    "    def get_evaluation_metrics(self, y_true: Union[pd.Series, np.array], y_pred_proba: Union[pd.Series, np.array]) -> Tuple[float, float, float, float]:\n",
    "        roc_auc = self.roc_auc(y_true, y_pred_proba)\n",
    "        pr_auc = self.pr_auc(y_true, y_pred_proba)\n",
    "        gini = self.gini(y_true, y_pred_proba)\n",
    "        ks = self.ks(y_true, y_pred_proba)\n",
    "        return (roc_auc, pr_auc, gini, ks)\n",
    "\n",
    "    def evaluate(self):\n",
    "        os.environ[\"MLFLOW_TRAKING_URI\"] = self.config.mlflow_uri\n",
    "        os.environ[\"MLFLOW_TRACKING_USERNAME\"] = self.config.mlflow_username\n",
    "        os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = self.config.mlflow_password\n",
    "\n",
    "        test = pd.read_csv(self.config.test_data_path)\n",
    "        model = joblib.load(self.config.model_path)\n",
    "\n",
    "        X_test = test.drop(columns=[self.config.target_column], axis=1)\n",
    "        y_test = test[self.config.target_column]\n",
    "\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, -1]\n",
    "            (roc_auc, pr_auc, gini, ks) = self.get_evaluation_metrics(y_test, y_pred_proba)\n",
    "            scores = {\n",
    "                'roc_auc': roc_auc,\n",
    "                'pr_auc': pr_auc,\n",
    "                'gini': gini,\n",
    "                'ks': ks\n",
    "            }\n",
    "            with open(self.config.metric_file_name, \"w\") as f:\n",
    "                json.dump(scores, f)\n",
    "            # mlflow.log_params(self.config.model_params)\n",
    "            mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "            mlflow.log_metric(\"pr_auc\", pr_auc)\n",
    "            mlflow.log_metric(\"gini\", gini)\n",
    "            mlflow.log_metric(\"ks\", ks)\n",
    "\n",
    "            if tracking_url_type_store != 'file':\n",
    "                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"WeightOfEvidence+LogisticRegression\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:27:26,987 - credit-scorecard-logger - INFO - yaml file: config.yaml loaded successfully\n",
      "2024-06-06 09:27:27,005 - credit-scorecard-logger - INFO - yaml file: params.yaml loaded successfully\n",
      "2024-06-06 09:27:27,005 - credit-scorecard-logger - INFO - yaml file: schema.yaml loaded successfully\n",
      "2024-06-06 09:27:27,005 - credit-scorecard-logger - INFO - Created directory at: artifacts\n",
      "2024-06-06 09:27:27,005 - credit-scorecard-logger - INFO - Created directory at: artifacts/model_evaluation\n",
      "2024-06-06 09:27:27,638 - credit-scorecard-logger - ERROR - INVALID_PARAMETER_VALUE: Response: {'error_code': 'INVALID_PARAMETER_VALUE'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    model_evaluation = ModelEvaluation( \n",
    "        config=configuration_manager.get_model_evaluation_config()\n",
    "    )\n",
    "    model_evaluation.evaluate()\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Restart and run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.common import logger\n",
    "from src.config.configuration_manager import ConfigurationManager\n",
    "from src.models.model_training import ModelTraining\n",
    "\n",
    "class ModelTrainingPipeline:\n",
    "    \"\"\"\n",
    "    Class to manage the model training pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Instantiate `ModelTrainingPipeline` class.\n",
    "        \"\"\"\n",
    "        self.configuration_manager = ConfigurationManager()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the model training process.\n",
    "        \"\"\"\n",
    "        model_training = ModelTraining(\n",
    "            config=self.configuration_manager.get_model_training_config()\n",
    "        )\n",
    "        model_training.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    STAGE_NAME = \"Model Training Stage\"\n",
    "    try:\n",
    "        logger.info(f\">>>>>> {STAGE_NAME} Started <<<<<<\")\n",
    "        model_training_pipeline = ModelTrainingPipeline()\n",
    "        model_training_pipeline.run()\n",
    "        logger.info(f\">>>>>> {STAGE_NAME} Completed <<<<<<\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk-modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
